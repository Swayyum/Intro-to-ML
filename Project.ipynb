{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Swayyum/Intro-to-ML--4105/blob/main/Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "NaBGndyhpKzX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(3 * 32 * 32, 512)\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 3 * 32 * 32)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "ek1aDB3UuAxE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6EJtYnzppVD",
        "outputId": "53490105-8250-49a4-d3e5-f9633a84fe25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Initialize the network and move it to the available device\n",
        "net = Net().to(device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Load and normalize CIFAR-10 data\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
        "                                         shuffle=False, num_workers=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roFabDq_pprA",
        "outputId": "1773c7ed-dfd0-4365-c2f2-6aee859816a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, loss: 1.6432463600313663\n",
            "Epoch 2, loss: 1.4793722655665875\n",
            "Epoch 3, loss: 1.3993698593208195\n",
            "Epoch 4, loss: 1.3418003875374793\n",
            "Epoch 5, loss: 1.288376087243855\n",
            "Epoch 6, loss: 1.2406752198442816\n",
            "Epoch 7, loss: 1.1956384915010632\n",
            "Epoch 8, loss: 1.1625211807837337\n",
            "Epoch 9, loss: 1.1204689954170586\n",
            "Epoch 10, loss: 1.086811033884138\n",
            "Epoch 11, loss: 1.047643265315108\n",
            "Epoch 12, loss: 1.0162884049049952\n",
            "Epoch 13, loss: 0.9860871710606665\n",
            "Epoch 14, loss: 0.9556568885951955\n",
            "Epoch 15, loss: 0.9240603047483741\n",
            "Epoch 16, loss: 0.89570480790223\n",
            "Epoch 17, loss: 0.871678037364576\n",
            "Epoch 18, loss: 0.8421698403464711\n",
            "Epoch 19, loss: 0.8234142772232159\n",
            "Epoch 20, loss: 0.7936013158536749\n",
            "Epoch 21, loss: 0.7767776564631157\n",
            "Epoch 22, loss: 0.7582606893514565\n",
            "Epoch 23, loss: 0.7269062200011246\n",
            "Epoch 24, loss: 0.7189409959903517\n",
            "Epoch 25, loss: 0.7027759736564688\n",
            "Epoch 26, loss: 0.6725900642235072\n",
            "Epoch 27, loss: 0.6830855551679841\n",
            "Epoch 28, loss: 0.6535271434059096\n",
            "Epoch 29, loss: 0.6399627819321189\n",
            "Epoch 30, loss: 0.6117344857439868\n",
            "Epoch 31, loss: 0.6093860255049712\n",
            "Epoch 32, loss: 0.5964587744875218\n",
            "Epoch 33, loss: 0.5873548549877983\n",
            "Epoch 34, loss: 0.5601453611352858\n",
            "Epoch 35, loss: 0.5609536415386657\n",
            "Epoch 36, loss: 0.550176696227922\n",
            "Epoch 37, loss: 0.5328164057773402\n",
            "Epoch 38, loss: 0.5118467711787993\n",
            "Epoch 39, loss: 0.5015787633001563\n",
            "Epoch 40, loss: 0.503584227433529\n",
            "Epoch 41, loss: 0.5295387958744207\n",
            "Epoch 42, loss: 0.48916918002544924\n",
            "Epoch 43, loss: 0.490211044721053\n",
            "Epoch 44, loss: 0.4968559935698317\n",
            "Epoch 45, loss: 0.4659433328918953\n",
            "Epoch 46, loss: 0.454856229840041\n",
            "Epoch 47, loss: 0.4746286776933182\n",
            "Epoch 48, loss: 0.48076564524087634\n",
            "Epoch 49, loss: 0.40969983818354483\n",
            "Epoch 50, loss: 0.40094990968309\n",
            "Epoch 51, loss: 0.44673945869039605\n",
            "Epoch 52, loss: 0.4429375138128775\n",
            "Epoch 53, loss: 0.42276858280603513\n",
            "Epoch 54, loss: 0.4299025263760714\n",
            "Epoch 55, loss: 0.43125399154915967\n",
            "Epoch 56, loss: 0.4539552172241657\n",
            "Epoch 57, loss: 0.43612559766706327\n",
            "Epoch 58, loss: 0.4299405246161875\n",
            "Epoch 59, loss: 0.3920415530971435\n",
            "Epoch 60, loss: 0.3995091024080548\n",
            "Epoch 61, loss: 0.4050723427055277\n",
            "Epoch 62, loss: 0.3858691280875312\n",
            "Epoch 63, loss: 0.3973178059330644\n",
            "Epoch 64, loss: 0.4162368905937862\n",
            "Epoch 65, loss: 0.3628689031491902\n",
            "Epoch 66, loss: 0.3764207468979144\n",
            "Epoch 67, loss: 0.40365802000930195\n",
            "Epoch 68, loss: 0.3738658809035836\n",
            "Epoch 69, loss: 0.3521409413910974\n",
            "Epoch 70, loss: 0.38458343858674865\n",
            "Epoch 71, loss: 0.4341477655309072\n",
            "Epoch 72, loss: 0.38485618135633487\n",
            "Epoch 73, loss: 0.38401284111427836\n",
            "Epoch 74, loss: 0.36311283304825587\n",
            "Epoch 75, loss: 0.34782896775249605\n",
            "Epoch 76, loss: 0.3451093040906525\n",
            "Epoch 77, loss: 0.4348555731529018\n",
            "Epoch 78, loss: 0.36120251879172094\n",
            "Epoch 79, loss: 0.2897664706049512\n",
            "Epoch 80, loss: 0.3435516359921786\n",
            "Epoch 81, loss: 0.3351882483375853\n",
            "Epoch 82, loss: 0.33674223322855845\n",
            "Epoch 83, loss: 0.3142684583378376\n",
            "Epoch 84, loss: 0.3396866026337029\n",
            "Epoch 85, loss: 0.3240281292431289\n",
            "Epoch 86, loss: 0.319913179833919\n",
            "Epoch 87, loss: 0.36100646843084966\n",
            "Epoch 88, loss: 0.42671836391434464\n",
            "Epoch 89, loss: 0.3646955536797996\n",
            "Epoch 90, loss: 0.38064675870569253\n",
            "Epoch 91, loss: 0.3682784034160421\n",
            "Epoch 92, loss: 0.3801899354859847\n",
            "Epoch 93, loss: 0.3740829317880158\n",
            "Epoch 94, loss: 0.3090166050365608\n",
            "Epoch 95, loss: 0.2841062471987749\n",
            "Epoch 96, loss: 0.34093148782120336\n",
            "Epoch 97, loss: 0.31568561659521377\n",
            "Epoch 98, loss: 0.3790144963047854\n",
            "Epoch 99, loss: 0.304757886256045\n",
            "Epoch 100, loss: 0.3717951681800054\n",
            "Finished Training. Training time:  2706.149720430374\n",
            "Accuracy of the network on the 10000 test images: 49.94 %\n"
          ]
        }
      ],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ExtendedNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ExtendedNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(3 * 32 * 32, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)  # First additional hidden layer\n",
        "        self.fc3 = nn.Linear(256, 128)  # Second additional hidden layer\n",
        "        self.fc4 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 3 * 32 * 32)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the extended network\n",
        "extended_net = ExtendedNet().to(device)\n"
      ],
      "metadata": {
        "id": "PV4-GmXf5c6-"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.SGD(extended_net.parameters(), lr=0.001, momentum=0.9)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "start_time = time.time()\n",
        "for epoch in range(300):  # Train for 300 epochs\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = extended_net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f'Epoch {epoch + 1}, loss: {running_loss / len(trainloader)}')\n",
        "\n",
        "end_time = time.time()\n",
        "training_time = end_time - start_time\n",
        "print('Finished Training. Training time: ', training_time)\n",
        "\n",
        "# Evaluate the network\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        outputs = extended_net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f'Accuracy of the network on the 10000 test images: {accuracy} %')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmTTIdFxH9OC",
        "outputId": "a2b564a0-9450-4f9a-bb15-c42ac395b3d0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, loss: 1.7026028989839554\n",
            "Epoch 2, loss: 1.436857608128786\n",
            "Epoch 3, loss: 1.3182978446948528\n",
            "Epoch 4, loss: 1.2291358497148752\n",
            "Epoch 5, loss: 1.1510321067491174\n",
            "Epoch 6, loss: 1.0779026654478907\n",
            "Epoch 7, loss: 1.0076793543700875\n",
            "Epoch 8, loss: 0.9462121615547687\n",
            "Epoch 9, loss: 0.8840463321231306\n",
            "Epoch 10, loss: 0.8293751696531289\n",
            "Epoch 11, loss: 0.7723015003020502\n",
            "Epoch 12, loss: 0.7227384398120991\n",
            "Epoch 13, loss: 0.67842433023063\n",
            "Epoch 14, loss: 0.6355776542249182\n",
            "Epoch 15, loss: 0.6001772181331273\n",
            "Epoch 16, loss: 0.5601992782180395\n",
            "Epoch 17, loss: 0.533564403904909\n",
            "Epoch 18, loss: 0.5062712176800811\n",
            "Epoch 19, loss: 0.4753723126956285\n",
            "Epoch 20, loss: 0.4592320305454712\n",
            "Epoch 21, loss: 0.4295187584688046\n",
            "Epoch 22, loss: 0.41330751791506715\n",
            "Epoch 23, loss: 0.3991670396108498\n",
            "Epoch 24, loss: 0.3867805602717878\n",
            "Epoch 25, loss: 0.3684956936541938\n",
            "Epoch 26, loss: 0.36776693762670565\n",
            "Epoch 27, loss: 0.33266750508201154\n",
            "Epoch 28, loss: 0.3352358079092861\n",
            "Epoch 29, loss: 0.3321180455385443\n",
            "Epoch 30, loss: 0.31595551256602505\n",
            "Epoch 31, loss: 0.3130402895643613\n",
            "Epoch 32, loss: 0.3090715044635036\n",
            "Epoch 33, loss: 0.2888161638989069\n",
            "Epoch 34, loss: 0.2968790305816954\n",
            "Epoch 35, loss: 0.2887419013435002\n",
            "Epoch 36, loss: 0.276551621491777\n",
            "Epoch 37, loss: 0.28099553173020936\n",
            "Epoch 38, loss: 0.26144245662233667\n",
            "Epoch 39, loss: 0.26662700334326733\n",
            "Epoch 40, loss: 0.28185225716769385\n",
            "Epoch 41, loss: 0.24985697481075986\n",
            "Epoch 42, loss: 0.25095199420157827\n",
            "Epoch 43, loss: 0.2502154870613596\n",
            "Epoch 44, loss: 0.25742273008025157\n",
            "Epoch 45, loss: 0.24539598333640505\n",
            "Epoch 46, loss: 0.2491767535702897\n",
            "Epoch 47, loss: 0.26454432726300336\n",
            "Epoch 48, loss: 0.239930969543357\n",
            "Epoch 49, loss: 0.23204101760530782\n",
            "Epoch 50, loss: 0.2575308222495848\n",
            "Epoch 51, loss: 0.24605914040244053\n",
            "Epoch 52, loss: 0.23923256560336545\n",
            "Epoch 53, loss: 0.23765889897762346\n",
            "Epoch 54, loss: 0.2289018184563423\n",
            "Epoch 55, loss: 0.22897381824253746\n",
            "Epoch 56, loss: 0.22378953677730917\n",
            "Epoch 57, loss: 0.2301167528869809\n",
            "Epoch 58, loss: 0.23043530807079182\n",
            "Epoch 59, loss: 0.23457484670774964\n",
            "Epoch 60, loss: 0.2194464075466319\n",
            "Epoch 61, loss: 0.23234663382486012\n",
            "Epoch 62, loss: 0.25372369136097167\n",
            "Epoch 63, loss: 0.24026813095325647\n",
            "Epoch 64, loss: 0.22813177969045234\n",
            "Epoch 65, loss: 0.24983574846450898\n",
            "Epoch 66, loss: 0.24884468249650363\n",
            "Epoch 67, loss: 0.23414920445912088\n",
            "Epoch 68, loss: 0.22046259165230087\n",
            "Epoch 69, loss: 0.2504577658351238\n",
            "Epoch 70, loss: 0.23352675650917298\n",
            "Epoch 71, loss: 0.22951280887099954\n",
            "Epoch 72, loss: 0.259117178292795\n",
            "Epoch 73, loss: 0.25316092192275663\n",
            "Epoch 74, loss: 0.23101129111274527\n",
            "Epoch 75, loss: 0.24658549273037872\n",
            "Epoch 76, loss: 0.24185698210177609\n",
            "Epoch 77, loss: 0.2420016105527728\n",
            "Epoch 78, loss: 0.23702447086153214\n",
            "Epoch 79, loss: 0.2255098280047816\n",
            "Epoch 80, loss: 0.225855247641419\n",
            "Epoch 81, loss: 0.23273483511723686\n",
            "Epoch 82, loss: 0.22120224882080058\n",
            "Epoch 83, loss: 0.2451640357482993\n",
            "Epoch 84, loss: 0.23598151356827984\n",
            "Epoch 85, loss: 0.24150246183743984\n",
            "Epoch 86, loss: 0.24021649077369636\n",
            "Epoch 87, loss: 0.24195377068517127\n",
            "Epoch 88, loss: 0.22638718362082444\n",
            "Epoch 89, loss: 0.22719169348401716\n",
            "Epoch 90, loss: 0.22829238062210358\n",
            "Epoch 91, loss: 0.23013656887650227\n",
            "Epoch 92, loss: 0.22309616075058214\n",
            "Epoch 93, loss: 0.2397536137982198\n",
            "Epoch 94, loss: 0.24123511893498345\n",
            "Epoch 95, loss: 0.24685433969276754\n",
            "Epoch 96, loss: 0.23726204859876024\n",
            "Epoch 97, loss: 0.23559508205652877\n",
            "Epoch 98, loss: 0.2416911022770231\n",
            "Epoch 99, loss: 0.2533898744358644\n",
            "Epoch 100, loss: 0.21816385125613807\n",
            "Epoch 101, loss: 0.22191945588044684\n",
            "Epoch 102, loss: 0.23576996761429297\n",
            "Epoch 103, loss: 0.23663744015752053\n",
            "Epoch 104, loss: 0.2774122219946074\n",
            "Epoch 105, loss: 0.24385686145568303\n",
            "Epoch 106, loss: 0.24244175198814516\n",
            "Epoch 107, loss: 0.24961368343599613\n",
            "Epoch 108, loss: 0.24792409512194535\n",
            "Epoch 109, loss: 0.24468533750907642\n",
            "Epoch 110, loss: 0.25423069787704655\n",
            "Epoch 111, loss: 0.25490866446203514\n",
            "Epoch 112, loss: 0.28167783535424223\n",
            "Epoch 113, loss: 0.2892879872741641\n",
            "Epoch 114, loss: 0.23246749782081025\n",
            "Epoch 115, loss: 0.2574463224530442\n",
            "Epoch 116, loss: 0.2506892960898524\n",
            "Epoch 117, loss: 0.26657452698317735\n",
            "Epoch 118, loss: 0.2340731934142729\n",
            "Epoch 119, loss: 0.22160140249684013\n",
            "Epoch 120, loss: 0.2597850343748625\n",
            "Epoch 121, loss: 0.2605230889526782\n",
            "Epoch 122, loss: 0.2525034506957203\n",
            "Epoch 123, loss: 0.2692884118229271\n",
            "Epoch 124, loss: 0.2549768220167725\n",
            "Epoch 125, loss: 0.2724519215191708\n",
            "Epoch 126, loss: 0.2817419258049707\n",
            "Epoch 127, loss: 0.2348199043487224\n",
            "Epoch 128, loss: 0.25042549825107546\n",
            "Epoch 129, loss: 0.28903674771824883\n",
            "Epoch 130, loss: 0.25216572051479647\n",
            "Epoch 131, loss: 0.23951144873357866\n",
            "Epoch 132, loss: 0.2697722129347597\n",
            "Epoch 133, loss: 0.22873615549947404\n",
            "Epoch 134, loss: 0.26805580958638137\n",
            "Epoch 135, loss: 0.271436778598698\n",
            "Epoch 136, loss: 0.30312645898639107\n",
            "Epoch 137, loss: 0.3052091030507357\n",
            "Epoch 138, loss: 0.2808636386198676\n",
            "Epoch 139, loss: 0.25818202459561324\n",
            "Epoch 140, loss: 0.27099830481338877\n",
            "Epoch 141, loss: 0.22602888045075112\n",
            "Epoch 142, loss: 0.2543652480649893\n",
            "Epoch 143, loss: 0.2720731659230155\n",
            "Epoch 144, loss: 0.25429247139515826\n",
            "Epoch 145, loss: 0.2350131794580649\n",
            "Epoch 146, loss: 0.2665849968851046\n",
            "Epoch 147, loss: 0.2691227652425053\n",
            "Epoch 148, loss: 0.26372915716994255\n",
            "Epoch 149, loss: 0.3016891213629959\n",
            "Epoch 150, loss: 0.2737663719121049\n",
            "Epoch 151, loss: 0.28048480738159787\n",
            "Epoch 152, loss: 0.3058259817785603\n",
            "Epoch 153, loss: 0.2725197250986909\n",
            "Epoch 154, loss: 0.26429907826520377\n",
            "Epoch 155, loss: 0.30590266177824027\n",
            "Epoch 156, loss: 0.2588345912542928\n",
            "Epoch 157, loss: 0.2996895922944631\n",
            "Epoch 158, loss: 0.25826369268659716\n",
            "Epoch 159, loss: 0.305835048000235\n",
            "Epoch 160, loss: 0.30762830573214217\n",
            "Epoch 161, loss: 0.3248547669080012\n",
            "Epoch 162, loss: 0.2934948750281864\n",
            "Epoch 163, loss: 0.29172565491605973\n",
            "Epoch 164, loss: 0.3073423856595756\n",
            "Epoch 165, loss: 0.29425647154204004\n",
            "Epoch 166, loss: 0.3248220730405884\n",
            "Epoch 167, loss: 0.3287133999834165\n",
            "Epoch 168, loss: 0.39204666395461724\n",
            "Epoch 169, loss: 0.354905077012424\n",
            "Epoch 170, loss: 0.29624688504131375\n",
            "Epoch 171, loss: 0.3300212333410327\n",
            "Epoch 172, loss: 0.2672672693341554\n",
            "Epoch 173, loss: 0.28677111418644863\n",
            "Epoch 174, loss: 0.28507746178490373\n",
            "Epoch 175, loss: 0.33973005256738376\n",
            "Epoch 176, loss: 0.32964606168772526\n",
            "Epoch 177, loss: 0.29318765729468416\n",
            "Epoch 178, loss: 0.291828682896573\n",
            "Epoch 179, loss: 0.3746037222671856\n",
            "Epoch 180, loss: 0.4307378724260576\n",
            "Epoch 181, loss: 0.35653743365755586\n",
            "Epoch 182, loss: 0.38127393503713697\n",
            "Epoch 183, loss: 0.34748546350245\n",
            "Epoch 184, loss: 0.3504228854347601\n",
            "Epoch 185, loss: 0.4145062981566075\n",
            "Epoch 186, loss: 0.3572991296285752\n",
            "Epoch 187, loss: 0.4019189034593884\n",
            "Epoch 188, loss: 0.35364317723274413\n",
            "Epoch 189, loss: 0.35075602211579027\n",
            "Epoch 190, loss: 0.3433141466736888\n",
            "Epoch 191, loss: 0.40764063242239484\n",
            "Epoch 192, loss: 0.47603516609206226\n",
            "Epoch 193, loss: 0.467095711265463\n",
            "Epoch 194, loss: 0.3995832277020523\n",
            "Epoch 195, loss: 0.40785380904120094\n",
            "Epoch 196, loss: 0.38023678313893083\n",
            "Epoch 197, loss: 0.3818579725824835\n",
            "Epoch 198, loss: 0.4185303951723088\n",
            "Epoch 199, loss: 0.41046437188860335\n",
            "Epoch 200, loss: 0.3883554240857993\n",
            "Epoch 201, loss: 0.4141362043879871\n",
            "Epoch 202, loss: 0.4095583704764869\n",
            "Epoch 203, loss: 0.5023283537231484\n",
            "Epoch 204, loss: 0.5657807681309958\n",
            "Epoch 205, loss: 0.4962278103876399\n",
            "Epoch 206, loss: 0.5453876062282058\n",
            "Epoch 207, loss: 0.5424051643252316\n",
            "Epoch 208, loss: 0.5431139847071407\n",
            "Epoch 209, loss: 0.48657369834311553\n",
            "Epoch 210, loss: 0.46437146025282594\n",
            "Epoch 211, loss: 0.605156192466228\n",
            "Epoch 212, loss: 0.6144822010936409\n",
            "Epoch 213, loss: 0.5231205654346546\n",
            "Epoch 214, loss: 0.5300227369304654\n",
            "Epoch 215, loss: 0.5845368590039646\n",
            "Epoch 216, loss: 0.5517507108748784\n",
            "Epoch 217, loss: 0.6749818464274083\n",
            "Epoch 218, loss: 0.7204193753912228\n",
            "Epoch 219, loss: 0.6716980192000349\n",
            "Epoch 220, loss: 0.6144818677654643\n",
            "Epoch 221, loss: 0.6528051287257386\n",
            "Epoch 222, loss: 0.7330737744915885\n",
            "Epoch 223, loss: 0.6861867926642229\n",
            "Epoch 224, loss: 0.7835828811421748\n",
            "Epoch 225, loss: 0.7056545704738268\n",
            "Epoch 226, loss: 0.8737670956259016\n",
            "Epoch 227, loss: 0.7294064342937515\n",
            "Epoch 228, loss: 0.7531934184433885\n",
            "Epoch 229, loss: 0.7507911153385357\n",
            "Epoch 230, loss: 0.75654369392186\n",
            "Epoch 231, loss: 0.7024954713722256\n",
            "Epoch 232, loss: 0.7299247449142408\n",
            "Epoch 233, loss: 0.78034158281838\n",
            "Epoch 234, loss: 0.8452629252680015\n",
            "Epoch 235, loss: 0.9158296672360525\n",
            "Epoch 236, loss: 0.917721243429507\n",
            "Epoch 237, loss: 0.9612817400330314\n",
            "Epoch 238, loss: 1.0041263200249582\n",
            "Epoch 239, loss: 0.9468503544375094\n",
            "Epoch 240, loss: 1.0286722126963768\n",
            "Epoch 241, loss: 1.0515989714542033\n",
            "Epoch 242, loss: 1.2681092497169841\n",
            "Epoch 243, loss: 1.1394131497417954\n",
            "Epoch 244, loss: 1.0559741529538027\n",
            "Epoch 245, loss: 1.0808728425597063\n",
            "Epoch 246, loss: 1.1623090486002299\n",
            "Epoch 247, loss: 1.1185011327473537\n",
            "Epoch 248, loss: 1.1363373637446308\n",
            "Epoch 249, loss: 1.1521347097720391\n",
            "Epoch 250, loss: 1.0629559321847595\n",
            "Epoch 251, loss: 1.3708382262783882\n",
            "Epoch 252, loss: 1.421314612004468\n",
            "Epoch 253, loss: 1.4747798599559756\n",
            "Epoch 254, loss: 1.4440129971982678\n",
            "Epoch 255, loss: 1.3440314575416283\n",
            "Epoch 256, loss: 1.3930510825041216\n",
            "Epoch 257, loss: 1.3172317986448476\n",
            "Epoch 258, loss: 1.227897142897396\n",
            "Epoch 259, loss: 1.4227351003208553\n",
            "Epoch 260, loss: 1.3924466830194815\n",
            "Epoch 261, loss: 1.4420362604159078\n",
            "Epoch 262, loss: 1.5155235447540107\n",
            "Epoch 263, loss: 1.6876294709835202\n",
            "Epoch 264, loss: 1.7093208843200107\n",
            "Epoch 265, loss: 1.5919949751930385\n",
            "Epoch 266, loss: 1.6002409052190447\n",
            "Epoch 267, loss: 1.767270092594492\n",
            "Epoch 268, loss: 1.6734104364420195\n",
            "Epoch 269, loss: 1.6137673637905823\n",
            "Epoch 270, loss: 1.8687042793788609\n",
            "Epoch 271, loss: 1.8023465301661277\n",
            "Epoch 272, loss: 1.7583075200973852\n",
            "Epoch 273, loss: 1.908780923404724\n",
            "Epoch 274, loss: 1.8014786883689273\n",
            "Epoch 275, loss: 1.9027884396702226\n",
            "Epoch 276, loss: 1.9051201978797756\n",
            "Epoch 277, loss: 1.809614315598507\n",
            "Epoch 278, loss: 1.842582593611348\n",
            "Epoch 279, loss: 1.8457417172348822\n",
            "Epoch 280, loss: 1.7875593078421255\n",
            "Epoch 281, loss: 1.800005964964534\n",
            "Epoch 282, loss: 1.7451956270103224\n",
            "Epoch 283, loss: 1.6993759683040777\n",
            "Epoch 284, loss: 1.745144566984266\n",
            "Epoch 285, loss: 1.6907603281561807\n",
            "Epoch 286, loss: 1.6547797120330274\n",
            "Epoch 287, loss: 1.7332202385753173\n",
            "Epoch 288, loss: 1.8346008071237232\n",
            "Epoch 289, loss: 1.9817918515492976\n",
            "Epoch 290, loss: 2.001329480657768\n",
            "Epoch 291, loss: 1.9171709615490236\n",
            "Epoch 292, loss: 1.9703133775369375\n",
            "Epoch 293, loss: 1.9061187150848447\n",
            "Epoch 294, loss: 1.921413431417046\n",
            "Epoch 295, loss: 1.8286254427032924\n",
            "Epoch 296, loss: 1.903104734380622\n",
            "Epoch 297, loss: 1.912942880533743\n",
            "Epoch 298, loss: 1.8234986850403998\n",
            "Epoch 299, loss: 1.8766170462882221\n",
            "Epoch 300, loss: 1.8208132937813037\n",
            "Finished Training. Training time:  9380.315206050873\n",
            "Accuracy of the network on the 10000 test images: 24.18 %\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPG65xMPQoGHbC7gd1wpNJS",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}